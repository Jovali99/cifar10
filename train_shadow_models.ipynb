{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import optuna\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from src.save_load import buildAuditMetadata, saveAudit\n",
    "from src.train_models import trainTargetModel\n",
    "from src.visualize_model import VisualizeModel\n",
    "from src.utils import print_yaml, calculate_logits_and_inmask\n",
    "from src.models.resnet18_model import ResNet18\n",
    "from src.cifar_handler import CifarInputHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d43da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#  Load target model and metadata  #\n",
    "# -------------------------------- #\n",
    "target_folder = \"resnet18-8bef88e056\"\n",
    "target_path = os.path.join(\"target\", target_folder)\n",
    "\n",
    "# Target Pickle Metadata .json\n",
    "metadata_pkl_path = os.path.join(target_path, \"model_metadata.pkl\")\n",
    "with open(metadata_pkl_path, \"rb\") as f:\n",
    "    metadata_pkl = pickle.load(f)\n",
    "    \n",
    "# Target Metadata .json\n",
    "metadata_path = os.path.join(target_path, \"metadata.json\")\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "#   Load Audit yaml   #\n",
    "# ------------------- #\n",
    "config = None\n",
    "with open(\"./audit.yaml\") as file:\n",
    "    audit_config = yaml.safe_load(file)\n",
    "\n",
    "print(\"-------------- Audit config --------------\")\n",
    "print_yaml(audit_config['audit'])\n",
    "print_yaml(audit_config['target'])\n",
    "\n",
    "# Update the audit config\n",
    "audit_config['target']['target_folder'] = target_path\n",
    "\n",
    "print(\"\\n-------------- Updated audit config --------------\")\n",
    "print_yaml(audit_config['audit'])\n",
    "print_yaml(audit_config['target'])\n",
    "\n",
    "# ------------------- #\n",
    "#   Load Train yaml   #\n",
    "# ------------------- #\n",
    "config = None\n",
    "with open(\"./train.yaml\") as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "    \n",
    "print(\"\\n-------------- Train config --------------\")\n",
    "print_yaml(train_config)\n",
    "\n",
    "# Update the train config with target metadata (learning rate, batch size, etc)\n",
    "train_config['train']['epochs'] = metadata['train']['epochs']\n",
    "train_config['train']['batch_size'] = metadata['train']['batch_size']\n",
    "train_config['train']['learning_rate'] = metadata['train']['learning_rate']\n",
    "train_config['train']['momentum'] = metadata['train']['momentum']\n",
    "\n",
    "train_config['run']['log_dir'] = target_path\n",
    "\n",
    "print(\"\\n-------------- Updated train config --------------\")\n",
    "print_yaml(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LeakPro.leakpro.schemas import LeakProConfig\n",
    "from LeakPro.leakpro.attacks.mia_attacks.lira import AttackLiRA\n",
    "from LeakPro.leakpro.attacks.utils.shadow_model_handler import ShadowModelHandler\n",
    "from LeakPro.leakpro.input_handler.mia_handler import MIAHandler\n",
    "from src.cifar_handler import CifarInputHandler\n",
    "# ----------------- #\n",
    "#   Setup LeakPro   #\n",
    "# ----------------- #\n",
    "\n",
    "# Intizializing\n",
    "leakpro_configs = LeakProConfig(**audit_config)\n",
    "print(\"-------- LeakPro Configs --------\")\n",
    "print_yaml(leakpro_configs)\n",
    "\n",
    "handler = MIAHandler(leakpro_configs, CifarInputHandler)\n",
    "\n",
    "configs = handler.configs.audit.attack_list[0]\n",
    "print(\"-------- Attack Configs --------\")\n",
    "print_yaml(configs)\n",
    "\n",
    "attack = AttackLiRA(handler=handler, configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#   Setup Shadow models Training   #\n",
    "# -------------------------------- #\n",
    "\n",
    "#Set number of shadow models to train\n",
    "num_shadow_models = configs[\"num_shadow_models\"]\n",
    "#Set online flag\n",
    "online = configs[\"online\"]\n",
    "\n",
    "attack_data_indices = attack.sample_indices_from_population(include_train_indices = online,\n",
    "                                                        include_test_indices = online)\n",
    "\n",
    "training_data_fraction = attack.training_data_fraction\n",
    "\n",
    "smh = ShadowModelHandler(handler)\n",
    "smh.epochs = train_config[\"train\"][\"epochs\"]\n",
    "smh.batch_size = train_config['train']['batch_size']\n",
    "smh.learning_rate = train_config['train']['learning_rate']\n",
    "smh.momentum = train_config['train']['momentum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf34207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "#   Train the Shadow models   #\n",
    "# --------------------------- #\n",
    "shadow_model_indices = smh.create_shadow_models(num_models = num_shadow_models,\n",
    "                                                 shadow_population =  attack_data_indices,\n",
    "                                                 training_fraction = training_data_fraction,\n",
    "                                                 online = online,\n",
    "                                                 #verbose = False,\n",
    "                                                 #incremental = INCREMENTAL, \n",
    "                                                 #shuffle_shift = SHUFFLE_SHIFT\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- #\n",
    "#   Load Shadow Models   #\n",
    "# ---------------------- #\n",
    "shadow_models, _ = smh.get_shadow_models(shadow_model_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audit dataset from the attack\n",
    "audit_dataset = attack.audit_dataset\n",
    "audit_data_indices = audit_dataset[\"data\"]\n",
    "true_labels = handler.get_labels(audit_dataset[\"data\"])\n",
    "print(f\"\\nTrue labels fetched: {true_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#   Extract Shadow model Signals   #\n",
    "# -------------------------------- #\n",
    "\n",
    "# Fetch and rescale shadow model logits\n",
    "shadow_models_logits = []\n",
    "for indx in shadow_model_indices:\n",
    "    shadow_models_logits.append(smh.load_logits(indx=indx))\n",
    "# Transpose the rescaled_sm_logits as rescale_logits flips them\n",
    "rescaled_sm_logits = np.array([attack.rescale_logits(x, true_labels) for x in shadow_models_logits]).T\n",
    "\n",
    "# Get the in indices mask for shadow models\n",
    "in_indices_masks = ShadowModelHandler(handler).get_in_indices_mask(shadow_model_indices, audit_dataset[\"data\"])#.astype(int)\n",
    "\n",
    "print(\"\\n--------- Shadow models logits extracted ---------\")\n",
    "print(f\"sm in mask:\\n {in_indices_masks[:10]}\")\n",
    "print(f\"Shadow model logits:\\n {rescaled_sm_logits[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ead28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#   Extract Target model Signals   #\n",
    "# -------------------------------- #\n",
    "# Logits\n",
    "target_logits = smh.load_logits(name=\"target\")\n",
    "rescaled_target_logits = attack.rescale_logits(target_logits, true_labels)\n",
    "\n",
    "#In mask\n",
    "train_indices = metadata_pkl.train_indices\n",
    "test_indices = metadata_pkl.test_indices\n",
    "target_audit_data_indices = np.concatenate([train_indices, test_indices])\n",
    "\n",
    "if(np.array_equal(audit_dataset[\"data\"], target_audit_data_indices)):\n",
    "    target_in_mask = np.isin(target_audit_data_indices, train_indices)\n",
    "else:\n",
    "    print(\"audit_dataset does not match target_audit_data_indices\")\n",
    "\n",
    "print(\"\\n--------- target model logits & in mask extracted ---------\")\n",
    "print(f\"target in mask:\\n {target_in_mask[:10]}\")\n",
    "print(f\"target logits:\\n {rescaled_target_logits[:10]}\")\n",
    "print(f\"target logits shape: {rescaled_target_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #\n",
    "#   Save logits & indices mask   #\n",
    "# ------------------------------ #\n",
    "metadata = buildAuditMetadata(train_config, audit_config)\n",
    "\n",
    "hash_id, save_dir = saveAudit(metadata, rescaled_target_logits, rescaled_sm_logits, in_indices_masks, target_in_mask, audit_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------- #\n",
    "#   STANDALONE Shadow Model, Metadata  and Dataset Loader used for calculating Logits and in_mask   #\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "# ---------------------------- #\n",
    "#   Load Dataset from Pickle   #\n",
    "# ---------------------------- #\n",
    "# HAVE TO GET DATASET FROM TARGET METADATA IF TO AUTOMATE\n",
    "dataset_name = \"cifar10\"\n",
    "print(f\"Dataset used by target model: {dataset_name}\")  \n",
    "\n",
    "reload_dataset = True\n",
    "if reload_dataset:\n",
    "    data_path = \"data\"\n",
    "    dataset_pkl_path = os.path.join(data_path, dataset_name + \".pkl\")    \n",
    "    with open(dataset_pkl_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # Wrap dataset if needed\n",
    "        \n",
    "    if not isinstance(dataset, CifarInputHandler.UserDataset):\n",
    "        data_tensor, target_tensor = dataset\n",
    "        dataset = CifarInputHandler.UserDataset(data_tensor, target_tensor)\n",
    "\n",
    "# ------------------------------------------------------- #\n",
    "#   Load Shadow Models and Calculate Logits and in_mask   #\n",
    "# ------------------------------------------------------- #\n",
    "# Path to the raw shadow models\n",
    "shadow_models_path = \"shadow_models/attack_objects/shadow_model\"\n",
    "\n",
    "# Path to processed shadow models\n",
    "sm_path = os.path.join(\"processed_shadow_models\", target_folder)\n",
    "os.makedirs(sm_path, exist_ok=True)\n",
    "\n",
    "reload_shadow_models = True\n",
    "if reload_shadow_models:\n",
    "    i = 0\n",
    "    while True:\n",
    "        model_pkl = os.path.join(shadow_models_path, f\"shadow_model_{i}.pkl\")\n",
    "        metadata_pkl = os.path.join(shadow_models_path, f\"metadata_{i}.pkl\")\n",
    "\n",
    "        if not os.path.exists(model_pkl) or not os.path.exists(metadata_pkl):\n",
    "            break  # stop when no more models\n",
    "\n",
    "        print(f\"Loading shadow model {i}\")\n",
    "\n",
    "        # Load model weights\n",
    "        state_dict = torch.load(model_pkl, map_location=\"cpu\")\n",
    "\n",
    "        # AVAILABLE PARAM CHECKS: model_class, online, init_params[\"num_classes\"\"]\n",
    "        # Reinstantiate model\n",
    "        num_classes = 10  # adapt if CIFAR-100\n",
    "        model = ResNet18(num_classes=num_classes)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        # Load metadata\n",
    "        with open(metadata_pkl, \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        calculate_logits_and_inmask(dataset, model, metadata, sm_path, idx=i)\n",
    "        \n",
    "        # Clean up\n",
    "        del metadata\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "print(\"\\nAll shadow model logits computed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

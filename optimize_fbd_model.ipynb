{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch import tensor, cat, save, load, optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models.resnet18_model import ResNet18\n",
    "\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "import src.study_handler as sh\n",
    "from src.utils import print_yaml, get_shadow_signals, percentile_score_normalization, rescale_logits\n",
    "from LeakPro.leakpro.attacks.mia_attacks.rmia import rmia_vectorised, rmia_get_gtlprobs\n",
    "from src.save_load import loadTargetSignals, loadShadowModelSignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. set device\n",
    "2. Load the study.yaml which contains the fbd_study key\n",
    "3. load baseline audit signals, shadow models logits, sm inmask and metadata\n",
    "4. Use metadata to update the training part of fbd_study\n",
    "5. Select sm to be used\n",
    "6. TODO Calc the vulnerability of the baseline model\n",
    "7. Normalize the vulnerability score\n",
    "8. Prepare the dataset using the baseline inmask to make sure we train on the baseline trainset\n",
    "9. Init the study and run it\n",
    "10. Visualize the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68780cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------#\n",
    "#  Set device  #\n",
    "# -------------#\n",
    "sh.DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {sh.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------#\n",
    "#  Load Study Config  #\n",
    "#---------------------#\n",
    "config = None\n",
    "with open(\"./study.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print_yaml(f\"Initial study config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "#  Load Model Signals  #\n",
    "#----------------------#\n",
    "target_folder = config[\"fbd_study\"][\"target_folder\"]\n",
    "\n",
    "# Load target signals\n",
    "target_logits, target_inmask, metadata = loadTargetSignals(target_folder)\n",
    "print(f\"Target logits and inmask shapes: {target_logits.shape}, {target_inmask.shape}\")\n",
    "\n",
    "# Load processed shadow model signals\n",
    "sm_logits, sm_inmask = loadShadowModelSignals(target_folder)\n",
    "print(f\"Target logits and inmask shapes: {sm_logits.shape}, {sm_inmask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------#\n",
    "#  Update Study Config  #\n",
    "#-----------------------#\n",
    "print_yaml(\"Initial study config:\")\n",
    "print_yaml(config['fbd_study'])\n",
    "train_metadata = metadata[\"train\"]\n",
    "\n",
    "config['fbd_study'][\"epochs\"] = train_metadata[\"epochs\"]\n",
    "config['fbd_study'][\"batch_size\"] = train_metadata[\"batch_size\"]\n",
    "config['fbd_study'][\"momentum\"] = train_metadata[\"momentum\"]\n",
    "config['fbd_study'][\"learning_rate\"] = train_metadata[\"learning_rate\"]\n",
    "config['fbd_study'][\"t_max\"] = train_metadata[\"t_max\"]\n",
    "config['fbd_study'][\"weight_decay\"] = train_metadata[\"weight_decay\"]\n",
    "\n",
    "print_yaml(\"\\nUpdated study config:\")\n",
    "print_yaml(config['fbd_study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce50bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "#  Select Shadow Models  #\n",
    "#------------------------#\n",
    "sm_count = config['fbd_study']['shadow_model_count']\n",
    "print(f\"Randomly selecting {sm_count} to be used for the study.\")\n",
    "shadow_logits, shadow_inmask = get_shadow_signals(sm_logits, sm_inmask, sm_count)\n",
    "print(f\"Shape of selected_sm_logits: {shadow_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------#\n",
    "#  Prepare dataset  #\n",
    "#-------------------#\n",
    "from src.dataset_handler import processDataset, loadDataset\n",
    "data_cfg = config['data']\n",
    "trainset, testset = loadDataset(data_cfg)\n",
    "\n",
    "# Will split the dataset to use the same in indices as the baseline target model\n",
    "train_dataset, test_dataset, train_indices, test_indices = processDataset(data_cfg, trainset, testset, in_indices_mask=target_inmask)\n",
    "\n",
    "# Retrieve the targets\n",
    "full_dataset = train_dataset.dataset\n",
    "targets = full_dataset.targets\n",
    "print(f\"Length of dataset targets/labels: {len(targets)}\")\n",
    "print(f\"First 10 targets/labels: {targets[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------#\n",
    "#  Calculate Vulnerability score using RMIA  #\n",
    "#--------------------------------------------#\n",
    "# Calculate the GTL Probabilities for shadow model logits\n",
    "N, M, C = shadow_logits.shape\n",
    "shadow_gtl_probs_list = []\n",
    "\n",
    "for m in range(M):\n",
    "    model_logits = shadow_logits[:, m, :]  # shape (N, C)\n",
    "    probs = rmia_get_gtlprobs(model_logits, targets)\n",
    "    shadow_gtl_probs_list.append(probs)\n",
    "    print(f\"{len(shadow_gtl_probs_list)} shadow gtl probs calculated\")\n",
    "\n",
    "shadow_gtl_probs = np.stack(shadow_gtl_probs_list, axis=1)  # shape = (N, M)\n",
    "\n",
    "# Calculate the GTL Probabilities for the target logits\n",
    "target_gtl_probs = rmia_get_gtlprobs(target_logits, targets)\n",
    "print(f\"Target gtl_probs: {target_gtl_probs[:10]}, shape: {target_gtl_probs.shape}\")\n",
    "\n",
    "rmia_scores = rmia_vectorised(target_gtl_probs, shadow_gtl_probs, shadow_inmask, online=True, use_gpu_if_available=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd945513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------#\n",
    "#  Calculate Rescaled shadow logits for LiRA  #\n",
    "#---------------------------------------------#\n",
    "rescaled_shadow_logits = np.zeros((N, M))  # Each column = rescaled logits for one shadow model\n",
    "\n",
    "for m in range(M):\n",
    "    rescaled_shadow_logits[:, m] = rescale_logits(shadow_logits[:, m, :], targets)\n",
    "\n",
    "print(f\"rescaled_shadow_logits shape: {rescaled_shadow_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the RMIA score distribution\n",
    "plt.hist(rmia_scores, bins=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97051e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------#\n",
    "#        Run study        #\n",
    "# ------------------------#\n",
    "import src.save_load as sl\n",
    "\n",
    "def run_optimization(config):\n",
    "    study_cfg = config['fbd_study']\n",
    "    \n",
    "    metadata = sl.buildStudyMetadata(study_cfg, config['data'])\n",
    "    _, save_path = sl.saveStudy(metadata, savePath=study_cfg['root'])\n",
    "    \n",
    "    journal_path = os.path.join(save_path, \"journal.log\")\n",
    "    storage = JournalStorage(JournalFileBackend(file_path=journal_path))\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=study_cfg[\"study_name\"],\n",
    "        storage=storage,\n",
    "        load_if_exists=True,\n",
    "        directions=[\"maximize\", \"minimize\"]\n",
    "    )\n",
    "    \n",
    "    func = lambda trial: sh.fbd_objective(trial, rmia_scores, train_dataset, test_dataset,\n",
    "                                       config, rescaled_shadow_logits, shadow_inmask, target_inmask)\n",
    "    \n",
    "    study.optimize(func, n_trials=study_cfg[\"trials\"])\n",
    "    \n",
    "    \n",
    "    print(f\"Study '{study_cfg['study_name']}' completed. Best value: {study.best_values}, params: {study.best_params}\")\n",
    "    \n",
    "    df = study.trials_dataframe()\n",
    "    df.to_csv(os.path.join(save_path, \"results.csv\"), index=False)\n",
    "    print(f\"ðŸ“„ Results saved to {os.path.join(save_path, 'results.csv')}\")\n",
    "\n",
    "    return study\n",
    "\n",
    "study = None\n",
    "if config is not None:\n",
    "    study = run_optimization(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_journal = True\n",
    "\n",
    "if load_from_journal:\n",
    "    journal_path = \"study/cifar10-resnet-fbd-lira-aa5cec7773/journal.log\"\n",
    "    \n",
    "    # Re-open storage\n",
    "    storage = JournalStorage(JournalFileBackend(journal_path))\n",
    "    \n",
    "    # Load the study by name (must match what was used originally)\n",
    "    study = optuna.load_study(\n",
    "        study_name=\"cifar10-resnet-fbd-lira\",\n",
    "        storage=storage\n",
    "    )\n",
    "    print(\"study loaded: cifar10-resnet-fbd-lira\")\n",
    "\n",
    "if study is not None:\n",
    "    print(\"visualizing study\")\n",
    "    fig1 = optuna.visualization.plot_pareto_front(study)\n",
    "    fig1.show()\n",
    "\n",
    "    fig2 = optuna.visualization.plot_param_importances(study)\n",
    "    fig2.show()\n",
    "else:\n",
    "    print(\"Study has not been run\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

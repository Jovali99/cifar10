{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0ae87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch import tensor, cat, save, load, optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.resnet18_model import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective\n",
    "def Objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -10, 10)\n",
    "    return x**2/(10-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fa0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(Objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ca3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best value:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------#\n",
    "#  Prepare dataset  #\n",
    "#-------------------#\n",
    "def loadDataset(data_cfg):\n",
    "    dataset_name = data_cfg[\"dataset\"]\n",
    "    root = data_cfg[\"data_dir\"]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    trainset, testset = None, None\n",
    "    if(dataset_name == \"cifar10\"):\n",
    "        trainset = CIFAR10(root=root, train=True, download=True, transform=transform)\n",
    "        testset = CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "    else:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    assert trainset != None, \"Failed loading the train set\"\n",
    "    assert testset != None, \"Failed loading the test set\"\n",
    "    print(\"-- Dataset loaded: \", dataset_name, \" --\")\n",
    "    return trainset, testset\n",
    "\n",
    "def processDataset(train_cfg, trainset, testset):\n",
    "    print(\"-- Processing dataset for training & auditing  --\")\n",
    "    \n",
    "    train_data, test_data, train_targets, test_targets = toTensor(trainset, testset)\n",
    "\n",
    "    assert train_data.shape[0] == 50000, \"Train should have 50000 samples\"\n",
    "    assert test_data.shape[0] == 10000, \"Test should have 10000 samples\"\n",
    "    assert train_data.shape[1] == 3, \"Train Data should have 3 channels\"\n",
    "    assert test_data.shape[1] == 3, \"Test Data should have 3 channels\"\n",
    "    assert train_data.max() >= 1 and train_data.min() >= 0, \"Train Data should be normalized\"\n",
    "    assert test_data.max() >= 1 and test_data.min() >= 0, \"Test Data should be normalized\"\n",
    "\n",
    "    data = cat([train_data.clone().detach(), test_data.clone().detach()], dim=0)\n",
    "    targets = cat([train_targets, test_targets], dim=0)\n",
    "\n",
    "    dataset = CifarInputHandler.UserDataset(data, targets)\n",
    "    dataset_size = len(dataset)\n",
    "    assert dataset_size == 60000, \"Population dataset should contain 60000 samples\"\n",
    "\n",
    "    data_attrib = train_cfg[\"data\"]\n",
    "    train_attrib = train_cfg[\"train\"]\n",
    "\n",
    "    dataset_name = data_attrib[\"dataset\"]\n",
    "    file_path = \"data/\" + dataset_name + \".pkl\"\n",
    "    saveDataset(dataset, file_path)\n",
    "    \n",
    "    train_frac = data_attrib[\"f_train\"]\n",
    "    test_frac = data_attrib[\"f_test\"]\n",
    "    batch_size = train_attrib[\"batch_size\"]\n",
    "\n",
    "    print(\"-- Preparing dataset loaders --\")\n",
    "    train_indices, test_indices = splitDataset(dataset, train_frac, test_frac)\n",
    "    train_loader, test_loader = prepareDataloaders(data, targets, train_indices, test_indices, batch_size)\n",
    "\n",
    "    return train_loader, test_loader, train_indices, test_indices\n",
    "\n",
    "with open(\"train.yaml\", \"r\") as file:\n",
    "        train_cfg = yaml.safe_load(file)\n",
    "\n",
    "trainset, testset = loadDataset(train_cfg[\"data\"])\n",
    "train_loader, test_loader, train_indices, test_indices = processDataset(train_cfg, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "#  Train baseline model  #\n",
    "#------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddc767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and config metadata. Folder name index-hashed_config/logits, metadata, etc\n",
    "form save-load import save, hashCfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train shadow models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0ae87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e7093",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Optuna objective\n",
    "def Objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -10, 10)\n",
    "    return x**2/(10-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fa0f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(Objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ca3d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best value:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f6148",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------#\n",
    "#  Prepare dataset  #\n",
    "#-------------------#\n",
    "def loadDataset(data_cfg):\n",
    "    dataset_name = data_cfg[\"dataset\"]\n",
    "    root = data_cfg[\"data_dir\"]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    trainset, testset = None, None\n",
    "    if(dataset_name == \"cifar10\"):\n",
    "        trainset = CIFAR10(root=root, train=True, download=True, transform=transform)\n",
    "        testset = CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "    else:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    assert trainset != None, \"Failed loading the train set\"\n",
    "    assert testset != None, \"Failed loading the test set\"\n",
    "    print(\"-- Dataset loaded: \", dataset_name, \" --\")\n",
    "    return trainset, testset\n",
    "\n",
    "with open(\"train.yaml\", \"r\") as file:\n",
    "        train_cfg = yaml.safe_load(file)\n",
    "\n",
    "trainset, testset = loadDataset(train_cfg[\"data\"])\n",
    "\n",
    "def processDataset(train_cfg, trainset, testset):\n",
    "    print(\"-- Processing dataset for training & auditing  --\")\n",
    "    \n",
    "    train_data, test_data, train_targets, test_targets = toTensor(trainset, testset)\n",
    "\n",
    "    assert train_data.shape[0] == 50000, \"Train should have 50000 samples\"\n",
    "    assert test_data.shape[0] == 10000, \"Test should have 10000 samples\"\n",
    "    assert train_data.shape[1] == 3, \"Train Data should have 3 channels\"\n",
    "    assert test_data.shape[1] == 3, \"Test Data should have 3 channels\"\n",
    "    assert train_data.max() >= 1 and train_data.min() >= 0, \"Train Data should be normalized\"\n",
    "    assert test_data.max() >= 1 and test_data.min() >= 0, \"Test Data should be normalized\"\n",
    "\n",
    "    data = cat([train_data.clone().detach(), test_data.clone().detach()], dim=0)\n",
    "    targets = cat([train_targets, test_targets], dim=0)\n",
    "\n",
    "    dataset = CifarInputHandler.UserDataset(data, targets)\n",
    "    dataset_size = len(dataset)\n",
    "    assert dataset_size == 60000, \"Population dataset should contain 60000 samples\"\n",
    "\n",
    "    data_attrib = train_cfg[\"data\"]\n",
    "    train_attrib = train_cfg[\"train\"]\n",
    "\n",
    "    dataset_name = data_attrib[\"dataset\"]\n",
    "    file_path = \"data/\" + dataset_name + \".pkl\"\n",
    "    saveDataset(dataset, file_path)\n",
    "    \n",
    "    train_frac = data_attrib[\"f_train\"]\n",
    "    test_frac = data_attrib[\"f_test\"]\n",
    "    batch_size = train_attrib[\"batch_size\"]\n",
    "\n",
    "    print(\"-- Preparing dataset loaders --\")\n",
    "    train_indices, test_indices = splitDataset(dataset, train_frac, test_frac)\n",
    "    train_loader, test_loader = prepareDataloaders(data, targets, train_indices, test_indices, batch_size)\n",
    "\n",
    "    return train_loader, test_loader, train_indices, test_indices\n",
    "\n",
    "with open(\"train.yaml\", \"r\") as file:\n",
    "        train_cfg = yaml.safe_load(file)\n",
    "\n",
    "trainset, testset = loadDataset(train_cfg[\"data\"])\n",
    "train_loader, test_loader, train_indices, test_indices = processDataset(train_cfg, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adae92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "#  Train baseline model  #\n",
    "#------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddc767",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save training and config metadata. Folder name index-hashed_config/logits, metadata, etc\n",
    "form save-load import save, hashCfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875fdc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train shadow models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
